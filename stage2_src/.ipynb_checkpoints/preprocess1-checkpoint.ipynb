{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from os.path import join as pjoin\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from multiprocess import Pool\n",
    "\n",
    "from others.logging import logger\n",
    "from others.tokenization import BertTokenizer\n",
    "from pytorch_transformers import XLNetTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from others.utils import clean\n",
    "from prepro.utils import _get_word_ngrams\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import xml.etree.ElementTree as ET\n",
    "from prepro.data_builder import BertTokenizer\n",
    "import nltk\n",
    "nyt_remove_words = [\"photo\", \"graph\", \"chart\", \"map\", \"table\", \"drawing\"]\n",
    "\"\"\"\n",
    "conda activate py3.6\n",
    "source /etc/profile\n",
    "export CLASSPATH=/tf/project/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar \n",
    "\"\"\"\n",
    "\n",
    "def recover_from_corenlp(s):\n",
    "    s = re.sub(r' \\'{\\w}', '\\'\\g<1>', s)\n",
    "    s = re.sub(r'\\'\\' {\\w}', '\\'\\'\\g<1>', s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(p, lower):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    flag = False\n",
    "    for sent in json.load(open(p))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        if (lower):\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        if (tokens[0] == '@highlight'):\n",
    "            flag = True\n",
    "            tgt.append([])\n",
    "            continue\n",
    "        if (flag):\n",
    "            tgt[-1].extend(tokens)\n",
    "        else:\n",
    "            source.append(tokens)\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,t = load_json(p='/tf/project/PreSumm-master/data/msmo_token/abb5721bac73e33bc733e3539520589e30832680.story.json', lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, large, temp_dir, finetune=False):\n",
    "        super(Bert, self).__init__()\n",
    "        if(large):\n",
    "            self.model = BertModel.from_pretrained('bert-large-uncased', cache_dir=temp_dir)\n",
    "        else:\n",
    "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
    "        \n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        if(self.finetune):\n",
    "            top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
    "        else:\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
    "        return top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertModel, BertConfig\n",
    "class get_bert_input():\n",
    "    def __init__(self,use_bert_basic_tokenizer=True,\n",
    "                 max_src_ntokens_per_sent=200,\n",
    "                 min_src_ntokens_per_sent=5,\n",
    "                 max_src_nsents=100,\n",
    "                 min_src_nsents=3,\n",
    "                max_tgt_ntokens=500,\n",
    "                min_tgt_ntokens=5,\n",
    "                lower=True):\n",
    "      \n",
    "        self.use_bert_basic_tokenizer=use_bert_basic_tokenizer\n",
    "        self.max_src_ntokens_per_sent=max_src_ntokens_per_sent\n",
    "        self.min_src_ntokens_per_sent=min_src_ntokens_per_sent\n",
    "        self.max_src_nsents=max_src_nsents\n",
    "        self.min_src_nsents=min_src_nsents\n",
    "        self.max_tgt_ntokens=max_tgt_ntokens\n",
    "        self.min_tgt_ntokens=min_tgt_ntokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.bert = Bert(False,'../temp', True).cuda()\n",
    "    def tokenized_sentence(self,sentence):\n",
    "        \n",
    "        return nltk.word_tokenize(sentence)\n",
    "    def process_sentence_bert(self,sentence):\n",
    "        sentence_token = [self.tokenized_sentence(sentence)[:self.max_src_ntokens_per_sent]]\n",
    "        sentence = [' '.join(sent) for sent in sentence_token]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(sentence)\n",
    "        \n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        print( src_subtokens)\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        print( src_subtokens)\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        \n",
    "       \n",
    "        return self.get_bert_vector(src_subtoken_idxs,[1]*len(src_subtoken_idxs),[True]*len(src_subtoken_idxs))\n",
    "    def get_bert_vector(self,src,seq,mask):\n",
    "        src = torch.Tensor(src).cuda()\n",
    "        seq = torch.Tensor(seq).cuda()\n",
    "        mask =  = torch.Tensor(mask).cuda()\n",
    "        return self.bert(src, segs, mask_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_bert_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'good', 'study', ',', 'day', 'day', 'up', '.']\n",
      "['[CLS]', 'good', 'good', 'study', ',', 'day', 'day', 'up', '.', '[SEP]']\n",
      "([101, 2204, 2204, 2817, 1010, 2154, 2154, 2039, 1012, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x.process_sentence_bert('good good study, day day up.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
