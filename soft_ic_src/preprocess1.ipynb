{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e4fa6b4842e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/home_project/PreSumm/soft_ic_src/others/tokenization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from os.path import join as pjoin\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from multiprocess import Pool\n",
    "\n",
    "from others.logging import logger\n",
    "from others.tokenization import BertTokenizer\n",
    "from pytorch_transformers import XLNetTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from others.utils import clean\n",
    "from prepro.utils import _get_word_ngrams\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import xml.etree.ElementTree as ET\n",
    "from prepro.data_builder import BertTokenizer\n",
    "import nltk\n",
    "nyt_remove_words = [\"photo\", \"graph\", \"chart\", \"map\", \"table\", \"drawing\"]\n",
    "\"\"\"\n",
    "conda activate py3.6\n",
    "source /etc/profile\n",
    "export CLASSPATH=/tf/project/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar \n",
    "\"\"\"\n",
    "\n",
    "def recover_from_corenlp(s):\n",
    "    s = re.sub(r' \\'{\\w}', '\\'\\g<1>', s)\n",
    "    s = re.sub(r'\\'\\' {\\w}', '\\'\\'\\g<1>', s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(p, lower):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    flag = False\n",
    "    for sent in json.load(open(p))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        if (lower):\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        if (tokens[0] == '@highlight'):\n",
    "            flag = True\n",
    "            tgt.append([])\n",
    "            continue\n",
    "        if (flag):\n",
    "            tgt[-1].extend(tokens)\n",
    "        else:\n",
    "            source.append(tokens)\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-417413267eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tf/project/PreSumm-master/data/msmo_token/abb5721bac73e33bc733e3539520589e30832680.story.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bbffaf5aeb1d>\u001b[0m in \u001b[0;36mload_json\u001b[0;34m(p, lower)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bbffaf5aeb1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean' is not defined"
     ]
    }
   ],
   "source": [
    "s,t = load_json(p='/tf/project/PreSumm-master/data/msmo_token/abb5721bac73e33bc733e3539520589e30832680.story.json', lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, large, temp_dir, finetune=False):\n",
    "        super(Bert, self).__init__()\n",
    "        if(large):\n",
    "            self.model = BertModel.from_pretrained('bert-large-uncased', cache_dir=temp_dir)\n",
    "        else:\n",
    "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
    "        \n",
    "        self.finetune = finetune\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        if(self.finetune):\n",
    "            top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
    "        else:\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                top_vec, _ = self.model(x, segs, attention_mask=mask)\n",
    "        return top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = Bert(False,'../temp', True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertModel, BertConfig\n",
    "class get_bert_input():\n",
    "    def __init__(self,use_bert_basic_tokenizer=True,\n",
    "                 max_src_ntokens_per_sent=200,\n",
    "                 min_src_ntokens_per_sent=5,\n",
    "                 max_src_nsents=100,\n",
    "                 min_src_nsents=3,\n",
    "                max_tgt_ntokens=500,\n",
    "                min_tgt_ntokens=5,\n",
    "                lower=True,\n",
    "                bert_model=None):\n",
    "      \n",
    "        self.use_bert_basic_tokenizer=use_bert_basic_tokenizer\n",
    "        self.max_src_ntokens_per_sent=max_src_ntokens_per_sent\n",
    "        self.min_src_ntokens_per_sent=min_src_ntokens_per_sent\n",
    "        self.max_src_nsents=max_src_nsents\n",
    "        self.min_src_nsents=min_src_nsents\n",
    "        self.max_tgt_ntokens=max_tgt_ntokens\n",
    "        self.min_tgt_ntokens=min_tgt_ntokens\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.bert = bert_model\n",
    "    def tokenized_sentence(self,sentence):\n",
    "        \n",
    "        return nltk.word_tokenize(sentence)\n",
    "    def process_sentence_bert(self,sentence):\n",
    "        sentence_token = [self.tokenized_sentence(sentence)[:self.max_src_ntokens_per_sent]]\n",
    "        sentence = [' '.join(sent) for sent in sentence_token]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(sentence)\n",
    "        \n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        print( src_subtokens)\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        print( src_subtokens)\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        return self.get_bert_vector(src_subtoken_idxs,[1]*len(src_subtoken_idxs),[True]*len(src_subtoken_idxs))\n",
    "    def get_bert_vector(self,src,seq,mask):\n",
    "        src = torch.Tensor(src).cuda().unsqueeze(0)\n",
    "        seq = torch.Tensor(seq).cuda().unsqueeze(0)\n",
    "        mask =  torch.Tensor(mask).cuda().unsqueeze(0)\n",
    "        return self.bert(src.long(), seq.long(), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_bert_input(bert_model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'good', 'study', ',', 'day', 'day', 'up', '.']\n",
      "['[CLS]', 'good', 'good', 'study', ',', 'day', 'day', 'up', '.', '[SEP]']\n",
      "tensor([[[-0.0231,  0.0306, -0.1991,  ..., -0.1348,  0.4759,  0.3542],\n",
      "         [-0.0090,  0.2746,  0.1730,  ..., -0.6673, -0.5767, -0.7292],\n",
      "         [ 0.0872,  0.1668,  0.0089,  ..., -0.3547, -0.3139, -0.8581],\n",
      "         ...,\n",
      "         [ 0.0624,  0.0135,  0.0500,  ...,  0.6453,  0.1331, -0.1702],\n",
      "         [ 0.8105,  0.0055, -0.3933,  ...,  0.1931, -0.4223, -0.4322],\n",
      "         [ 1.0457, -0.0533, -0.3945,  ...,  0.1900, -0.4931, -0.3089]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(x.process_sentence_bert('good good study, day day up.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e710be6d06a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tf/home_project/PreSumm/soft_ic_src/models/model_builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxavier_uniform_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'"
     ]
    }
   ],
   "source": [
    "from models import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e6d3a820c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_builder' is not defined"
     ]
    }
   ],
   "source": [
    "ext_model = model_builder.ExtSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
